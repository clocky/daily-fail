{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Fail\n",
    "\n",
    "Import required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DATA_FILE = \"../data/headlines.txt\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `requests` to download latest headlines with `BeautifulSoup`, then dedupe any new headlines from the existing headline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(\"https://www.dailymail.co.uk/news/headlines/index.html\")\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    sidebar = soup.select(\"ul.link-bogr2 li span.pufftext strong\")\n",
    "    new_headlines = []\n",
    "    for headline in sidebar:\n",
    "        new_headlines.append(headline.get_text(strip=True))\n",
    "    \n",
    "    with open(DATA_FILE, \"r\") as file:\n",
    "        existing_headlines = file.readlines()[-250:]\n",
    "        existing_headlines = [line.strip() for line in existing_headlines]\n",
    "        \n",
    "    new_headlines = [s for s in new_headlines if s not in existing_headlines]\n",
    "\n",
    "    with open(DATA_FILE, \"a\") as file:\n",
    "        for s in new_headlines:\n",
    "            file.write(s + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: \", response.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in existing headlines from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "filename = DATA_FILE\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    existing_headlines = file.readlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through file to identify NER's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counts = Counter()\n",
    "for string in existing_headlines:\n",
    "    doc = nlp(string.strip())\n",
    "    for ent in doc.ents:\n",
    "        entity_counts[(ent.text, ent.label_)] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a Pandas DataFrame from the Counter object\n",
    "2. Split the \"Entity\" column into two separate columns \"Text\" and \"Label\"\n",
    "3. Drop the \"Entity\" column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>38</td>\n",
       "      <td>Vanity Fair</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>Oscars</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>13</td>\n",
       "      <td>Oscar</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>12</td>\n",
       "      <td>BBC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>Brendan Fraser</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>11</td>\n",
       "      <td>Vanity Fair Oscars Party</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Michelle Yeoh</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>Academy Awards</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>9</td>\n",
       "      <td>Beverly Hills</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>the Vanity Fair Oscar Party</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>Jamie Lee Curtis</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8</td>\n",
       "      <td>Olivia</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>8</td>\n",
       "      <td>Vanity Fair's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Jimmy Kimmel</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Ashley Graham</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6</td>\n",
       "      <td>Best Actress</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6</td>\n",
       "      <td>Cara Delevingne</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Count                         Text   Label\n",
       "62      38                  Vanity Fair     ORG\n",
       "16      21                       Oscars     ORG\n",
       "10      17                    Hollywood     GPE\n",
       "57      13                        Oscar  PERSON\n",
       "116     12                          BBC     ORG\n",
       "5       12               Brendan Fraser  PERSON\n",
       "55      11     Vanity Fair Oscars Party     ORG\n",
       "3       10                Michelle Yeoh  PERSON\n",
       "69       9               Academy Awards     ORG\n",
       "42       9                Beverly Hills     GPE\n",
       "14       9                    Lady Gaga  PERSON\n",
       "15       9                      Rihanna  PERSON\n",
       "41       9  the Vanity Fair Oscar Party     ORG\n",
       "20       8             Jamie Lee Curtis  PERSON\n",
       "63       8                       Olivia     GPE\n",
       "520      8                Vanity Fair's     ORG\n",
       "6        7                 Jimmy Kimmel  PERSON\n",
       "0        6                Ashley Graham  PERSON\n",
       "30       6                 Best Actress     ORG\n",
       "45       6              Cara Delevingne  PERSON"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(entity_counts.items(), columns=['Entity', 'Count'], index=None)\n",
    "df[['Text', 'Label']] = pd.DataFrame(df['Entity'].tolist(), index=df.index)\n",
    "df = df.drop('Entity', axis=1)\n",
    "df = df.loc[df['Label'].isin(['PERSON', 'ORG', 'GPE', 'WORK_OF_ART'])]\n",
    "df = df.sort_values(by=['Count', 'Text'], ascending=[False, True])\n",
    "df = df.head(20)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
