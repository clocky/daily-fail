{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Fail\n",
    "\n",
    "Import required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DATA_FILE = \"../data/headlines.txt\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `requests` to download latest headlines with `BeautifulSoup`, then dedupe any new headlines from the existing headline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(\"https://www.dailymail.co.uk/news/headlines/index.html\")\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    sidebar = soup.select(\"ul.link-bogr2 li span.pufftext strong\")\n",
    "    new_headlines = []\n",
    "    for headline in sidebar:\n",
    "        new_headlines.append(headline.get_text(strip=True))\n",
    "    \n",
    "    with open(DATA_FILE, \"r\") as file:\n",
    "        existing_headlines = file.readlines()\n",
    "        existing_headlines = [line.strip() for line in existing_headlines]\n",
    "        \n",
    "    new_headlines = [s for s in new_headlines if s not in existing_headlines]\n",
    "\n",
    "    with open(DATA_FILE, \"a\") as file:\n",
    "        for s in new_headlines:\n",
    "            file.write(s + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: \", response.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in existing headlines from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "filename = DATA_FILE\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    existing_headlines = file.readlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through file to identify NER's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counts = Counter()\n",
    "for string in existing_headlines:\n",
    "    doc = nlp(string.strip())\n",
    "    for ent in doc.ents:\n",
    "        entity_counts[(ent.text, ent.label_)] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and display the data\n",
    "\n",
    "1. Create a Pandas DataFrame from the Counter object\n",
    "2. Split the \"Entity\" column into two separate columns \"Text\" and \"Label\"\n",
    "3. Drop the \"Entity\" column\n",
    "4. Filter out results to only show PERSON, ORG, GPE (General Point of Interest),\n",
    "   and WORK_OF_ART entities.\n",
    "5. Sort DataFrame by Count (largest first), then Text, alphabetically.\n",
    "6. Reduce the table to the top 20 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>Oscars</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>26</td>\n",
       "      <td>Oscar</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>14</td>\n",
       "      <td>BBC</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>12</td>\n",
       "      <td>Vanity Fair Oscars</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>Brendan Fraser</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>Michelle Yeoh</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10</td>\n",
       "      <td>Academy Awards</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10</td>\n",
       "      <td>Vanity Fair</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>9</td>\n",
       "      <td>Beverly Hills</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>Jimmy Kimmel</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>8</td>\n",
       "      <td>Maya Jama</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>8</td>\n",
       "      <td>Oscars</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>8</td>\n",
       "      <td>Vanity Fair's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Ashley Graham</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7</td>\n",
       "      <td>Cara Delevingne</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>7</td>\n",
       "      <td>Gigi Hadid</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7</td>\n",
       "      <td>Jamie Lee Curtis</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Count                Text        Label\n",
       "17      51              Oscars          ORG\n",
       "57      26               Oscar       PERSON\n",
       "13      17           Hollywood          GPE\n",
       "118     14                 BBC          ORG\n",
       "62      12  Vanity Fair Oscars          ORG\n",
       "6       11      Brendan Fraser       PERSON\n",
       "3       11       Michelle Yeoh       PERSON\n",
       "68      10      Academy Awards          ORG\n",
       "19      10             Rihanna       PERSON\n",
       "142     10         Vanity Fair          ORG\n",
       "41       9       Beverly Hills          GPE\n",
       "18       9           Lady Gaga       PERSON\n",
       "9        8        Jimmy Kimmel       PERSON\n",
       "193      8           Maya Jama       PERSON\n",
       "242      8              Oscars  WORK_OF_ART\n",
       "486      8       Vanity Fair's          ORG\n",
       "1        7       Ashley Graham       PERSON\n",
       "44       7     Cara Delevingne       PERSON\n",
       "133      7          Gigi Hadid       PERSON\n",
       "49       7    Jamie Lee Curtis       PERSON"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(entity_counts.items(), columns=['Entity', 'Count'], index=None)\n",
    "df[['Text', 'Label']] = pd.DataFrame(df['Entity'].tolist(), index=df.index)\n",
    "df = df.drop('Entity', axis=1)\n",
    "df = df.loc[df['Label'].isin(['PERSON', 'ORG', 'GPE', 'WORK_OF_ART'])]\n",
    "df = df.sort_values(by=['Count', 'Text'], ascending=[False, True])\n",
    "df = df.head(20)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
